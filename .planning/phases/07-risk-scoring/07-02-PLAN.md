---
phase: 07-risk-scoring
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - agents/prompts/risk-scorer.ts
  - agents/risk-scorer.ts
  - agents/tools/vector-search.ts
autonomous: true

must_haves:
  truths:
    - "Risk scorer prompt includes perspective-specific instructions (receiving/disclosing/balanced)"
    - "Risk scorer retrieves evidence from three sources: CUAD references, template baselines, and NLI spans"
    - "Risk scorer outputs structured references with sourceId enabling verification"
    - "Assessments include negotiation suggestions for non-standard clauses"
    - "Atypical language flagged even when substance is standard"
    - "LLM-generated citations are verified against reference database; hallucinated sourceIds are stripped"
    - "Evidence helpers return empty arrays gracefully when reference data is not populated"
  artifacts:
    - path: "agents/prompts/risk-scorer.ts"
      provides: "Perspective-aware system prompt and enhanced user prompt"
      contains: "createRiskScorerSystemPrompt"
    - path: "agents/risk-scorer.ts"
      provides: "Multi-source evidence retrieval and enhanced LLM call"
      contains: "retrieveEvidence"
    - path: "agents/tools/vector-search.ts"
      provides: "findTemplateBaselines and findNliSpans helpers"
      contains: "findTemplateBaselines"
  key_links:
    - from: "agents/risk-scorer.ts"
      to: "agents/prompts/risk-scorer.ts"
      via: "import createRiskScorerSystemPrompt"
      pattern: "createRiskScorerSystemPrompt"
    - from: "agents/risk-scorer.ts"
      to: "agents/tools/vector-search.ts"
      via: "import findTemplateBaselines, findNliSpans"
      pattern: "findTemplateBaselines|findNliSpans"
---

<objective>
Implement perspective-aware risk scoring with multi-source evidence retrieval and enhanced prompts.

Purpose: This is the core agent logic. The risk scorer must compare clauses against baselines from multiple sources, assess risk from the user's perspective, provide structured citations, and include negotiation suggestions.
Output: Working risk scorer agent that produces perspective-aware assessments with structured evidence from CUAD, templates, and NLI sources.
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-risk-scoring/07-CONTEXT.md
@.planning/phases/07-risk-scoring/07-RESEARCH.md
@.planning/phases/07-risk-scoring/07-01-SUMMARY.md
@agents/risk-scorer.ts
@agents/prompts/risk-scorer.ts
@agents/tools/vector-search.ts
@db/queries/similarity.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor prompts and add multi-source evidence helpers</name>
  <files>agents/prompts/risk-scorer.ts, agents/tools/vector-search.ts</files>
  <action>
**Part A: Refactor `agents/prompts/risk-scorer.ts`**

Replace the existing prompt with perspective-aware versions:

1. Replace `RISK_SCORER_SYSTEM_PROMPT` constant with a function `createRiskScorerSystemPrompt(perspective: Perspective)`:
   - Keep the risk level definitions from the existing prompt
   - Add perspective-specific instructions based on 07-CONTEXT.md decisions:
     - `receiving`: Assess from receiving party perspective. Clauses favoring disclosing party = higher risk.
     - `disclosing`: Assess from disclosing party perspective. Clauses giving receiving party too much latitude = higher risk.
     - `balanced`: Neutral perspective. One-sided clauses in either direction = higher risk.
   - Add explanation requirements from CONTEXT.md:
     - Risk-first pattern: lead with risk implication
     - VP of Sales audience (professional accessible)
     - Concrete negotiation suggestion for non-standard clauses
     - Flag atypical language even when substance is standard
     - 2-3 sentences maximum
   - Add evidence requirements matching the `enhancedRiskAssessmentSchema`:
     - Quote specific text from clause (citations)
     - Reference corpus matches with source labels (references)
     - Compare to template baseline when available (baselineComparison)
     - When no match: "No reference corpus match - assessment based on legal analysis only"
   - Include the JSON schema structure so the LLM knows what to output

2. Replace `createRiskScorerPrompt` with `createEnhancedRiskScorerPrompt`:
   - Parameters: `clauseText, category, references, templates, nliSpans, perspective`
   - References block: format each reference with `[REF-N] Source: {source} | Category: {category} | ID: {sourceId} | Similarity: {similarity}%\n{content (truncated to 300 chars)}`
   - Templates block: format with `[TPL-N] Source: {source} | ID: {sourceId} | Similarity: {similarity}%\n{content (truncated to 300 chars)}`
   - NLI block: format with `[NLI-N] Source: ContractNLI | Category: {category} | ID: {sourceId}\n{content (truncated to 200 chars)}`
   - Each block falls back to "No X available." when empty
   - End with: "Assess the risk level from the {perspective} perspective. Return JSON only."

3. Export both functions. Also export `RISK_SCORER_SYSTEM_PROMPT` as a backward-compatible alias:
```typescript
export const RISK_SCORER_SYSTEM_PROMPT = createRiskScorerSystemPrompt('balanced')
```

**Part B: Add evidence retrieval helpers to `agents/tools/vector-search.ts`**

Add two new exported helper functions after `findSimilarClauses`:

1. `findTemplateBaselines(clauseText, options?)`:
   - Generate embedding for clauseText using `voyageClient.embed(clauseText, 'query')`
   - Call `findSimilarReferences` from `@/db/queries/similarity` with `granularity: 'template'`, limit 2, threshold 0.5
   - Map results to `VectorSearchResult[]` format (id, content, category, similarity, source: 'bonterms' or 'commonaccord')
   - Cache results using the existing `searchCache`
   - Handle gracefully when no template data exists (return empty array)

2. `findNliSpans(clauseText, options?)`:
   - Generate embedding for clauseText using `voyageClient.embed(clauseText, 'query')`
   - Call `findSimilarReferences` from `@/db/queries/similarity` with `granularity: 'span'`, limit 2, threshold 0.5, and optional `category` filter
   - Map results to `VectorSearchResult[]` format (id, content, category, similarity, source: 'contract_nli')
   - Cache results using the existing `searchCache`
   - Handle gracefully when no NLI data exists (return empty array)

Import `findSimilarReferences` from `@/db/queries/similarity` at the top of the file.
  </action>
  <verify>
    - `pnpm build` succeeds
    - `pnpm lint` passes
    - `createRiskScorerSystemPrompt('balanced')` produces a string containing "balanced"
    - `createRiskScorerSystemPrompt('receiving')` contains "receiving party"
  </verify>
  <done>
    - Perspective-aware system prompt function exists
    - Enhanced user prompt includes three evidence source blocks (references, templates, NLI)
    - `findTemplateBaselines` and `findNliSpans` helper functions exported
    - Backward-compatible `RISK_SCORER_SYSTEM_PROMPT` export preserved
    - Evidence helpers return empty arrays when reference data is not populated (Phase 0 bootstrap prerequisite)
    - Verified: helpers handle empty `referenceEmbeddings` table gracefully (no throw, returns `[]`)
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement multi-source evidence retrieval and enhanced scoring loop</name>
  <files>agents/risk-scorer.ts</files>
  <action>
Refactor `runRiskScorerAgent` in `agents/risk-scorer.ts` to use the new prompts and multi-source evidence:

1. Update imports:
   - Import `createRiskScorerSystemPrompt, createEnhancedRiskScorerPrompt` from `./prompts`
   - Import `findTemplateBaselines, findNliSpans` from `./tools/vector-search`
   - Import `enhancedRiskAssessmentSchema, type Perspective` from `./types`

2. Refactor the main scoring loop in `runRiskScorerAgent`:

For each clause:
a. **Multi-source evidence retrieval** (in parallel):
```typescript
const [references, templates, nliSpans] = await Promise.all([
  findSimilarClauses(clause.clauseText, { category: clause.category, limit: 3 }),
  findTemplateBaselines(clause.clauseText, { limit: 2 }),
  findNliSpans(clause.clauseText, { category: clause.category, limit: 2 }),
])
```

b. **Build enhanced prompt**:
```typescript
const systemPrompt = createRiskScorerSystemPrompt(perspective)
const prompt = createEnhancedRiskScorerPrompt(
  clause.clauseText,
  clause.category,
  references,
  templates,
  nliSpans,
  perspective,
)
```

c. **Generate with enhanced schema**:
```typescript
const result = await generateText({
  model: getAgentModel('riskScorer'),
  system: systemPrompt,
  prompt,
  output: Output.object({ schema: enhancedRiskAssessmentSchema }),
})
```

d. **Verify citations against reference database (RSK-05)**:
After the LLM returns structured output, verify each `evidence.references[].sourceId` actually exists in the reference database:
```typescript
const verifiedReferences = await verifyCitations(output.evidence.references)
// verifyCitations queries reference_documents or reference_embeddings for each sourceId
// Returns only references where the sourceId was found
// Logs a warning for any hallucinated sourceIds: console.warn(`Hallucinated citation stripped: ${ref.sourceId}`)
```

Create a local `verifyCitations` helper in this file:
- Accept array of `{ sourceId, source, ... }` references
- For each reference, query `referenceDocuments` (from `@/db/schema/reference`) by `id = sourceId`
- If the record exists, keep the reference. If not, log a warning and filter it out.
- Batch the lookups: collect all sourceIds, do a single `WHERE id IN (...)` query, then filter
- Import `referenceDocuments` from `@/db/schema/reference` and `inArray` from `drizzle-orm`
- Accept the db client as a parameter (passed from the pipeline's tenant context)

e. **Map output to RiskAssessmentResult** (using verified references):
```typescript
assessments.push({
  clauseId: clause.chunkId,
  clause,
  riskLevel: output.riskLevel,
  confidence: output.confidence,
  explanation: output.explanation,
  negotiationSuggestion: output.negotiationSuggestion,
  atypicalLanguage: output.atypicalLanguage,
  atypicalLanguageNote: output.atypicalLanguageNote,
  evidence: {
    ...output.evidence,
    references: verifiedReferences, // Use verified references, not raw LLM output
  },
  startPosition: clause.startPosition,
  endPosition: clause.endPosition,
})
```

3. After the loop, compute the new output fields:
   - `perspective`: use `input.perspective ?? 'balanced'`
   - `riskDistribution`: count assessments per risk level
   - `executiveSummary`: generate via `generateExecutiveSummary(assessments, score, level)` (see below)

4. Add `generateExecutiveSummary` function (local to this file):
   - Sort assessments by severity (aggressive first, then cautious, unknown, standard)
   - Take top 3-5 non-standard clauses as key findings
   - Count risk level distribution
   - Format: "Overall Risk: {level} ({score}/100). {N} clauses analyzed: {counts}.\n\nKey Findings:\n1. {category}: {explanation}\n..."
   - Return the summary string

5. The existing `calculateOverallRisk` stays as-is for now (weighted scoring comes in Plan 03).

6. **Token budget awareness**: Before evidence retrieval, check `budgetTracker.isWarning`. If approaching limit, reduce reference limits (2 refs, 1 template, 1 NLI instead of 3/2/2). This prevents budget exhaustion on large documents.
  </action>
  <verify>
    - `pnpm build` succeeds
    - `pnpm lint` passes
    - The `runRiskScorerAgent` function signature accepts `perspective` in input
    - Output includes `executiveSummary`, `perspective`, `riskDistribution`
  </verify>
  <done>
    - Risk scorer retrieves evidence from CUAD references, template baselines, and NLI spans
    - LLM call uses enhanced schema with structured citations
    - LLM-generated sourceIds verified against reference_documents table; hallucinated citations stripped with warning log
    - Output includes perspective, executive summary, and risk distribution
    - Budget-aware reference count reduction when approaching token limit
    - Error handling preserved (NoObjectGeneratedError -> AnalysisFailedError)
  </done>
</task>

</tasks>

<verification>
- `pnpm build` succeeds
- `pnpm lint` passes
- Risk scorer agent accepts perspective parameter and uses it in prompt
- Evidence retrieval queries three sources (CUAD, templates, NLI)
- Output schema includes structured citations with sourceId/source fields
- Executive summary generated from assessment results
</verification>

<success_criteria>
- Risk scorer prompt is perspective-aware with three perspective modes
- Multi-source evidence retrieval (CUAD + templates + NLI) wired into scoring loop
- Enhanced schema used for LLM output with structured citations
- Negotiation suggestions and atypical language detection included
- Budget-aware reference count reduction
</success_criteria>

<output>
After completion, create `.planning/phases/07-risk-scoring/07-02-SUMMARY.md`
</output>
