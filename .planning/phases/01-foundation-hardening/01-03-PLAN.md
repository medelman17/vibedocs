---
phase: 01-foundation-hardening
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - inngest/functions/analyze-nda.ts
  - db/schema/analyses.ts
  - db/migrations/add-clause-extraction-constraint.sql
autonomous: true

must_haves:
  truths:
    - "Pipeline halts with clear error when parser or classifier validation fails"
    - "Database writes use deterministic IDs - retrying a step does not create duplicate records"
    - "Validation failures surface as user-visible errors (not silent progression)"
    - "Analysis record has status='failed' when validation fails"
    - "0 clauses = always halt (per CONTEXT.md)"
  artifacts:
    - path: "inngest/functions/analyze-nda.ts"
      provides: "Pipeline with validation gates and idempotent writes"
      contains: "validateParserOutput"
    - path: "inngest/functions/analyze-nda.ts"
      provides: "Deterministic analysis ID generation"
      contains: "createHash"
    - path: "db/schema/analyses.ts"
      provides: "Unique constraint on clauseExtractions"
      contains: "unique"
  key_links:
    - from: "inngest/functions/analyze-nda.ts"
      to: "agents/validation"
      via: "imports validation gates"
      pattern: "import.*validateParserOutput.*validateClassifierOutput"
    - from: "inngest/functions/analyze-nda.ts"
      to: "inngest/utils/errors"
      via: "imports NonRetriableError"
      pattern: "NonRetriableError"
    - from: "inngest/functions/analyze-nda.ts"
      to: "drizzle-orm"
      via: "imports eq for where clause"
      pattern: "import.*eq.*from.*drizzle-orm"
---

<objective>
Integrate validation gates and implement idempotent database writes in the pipeline.

Purpose: This plan wires together the validation infrastructure (Plan 01) into the analyze-nda.ts pipeline, adds upsert patterns for safe retries, and ensures validation failures become user-visible errors with proper status tracking.

Output:
- analyze-nda.ts with validation gates after parser and classifier steps
- Upsert patterns for analysis creation and clause persistence
- Unique constraint on clauseExtractions table for idempotent inserts
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-hardening/01-RESEARCH.md (validation gate integration, upsert patterns)
@.planning/phases/01-foundation-hardening/01-01-SUMMARY.md (validation gate exports)
@.planning/phases/01-foundation-hardening/01-02-SUMMARY.md (migrated agents)

Source files to modify:
@inngest/functions/analyze-nda.ts
@db/schema/analyses.ts
@inngest/utils/errors.ts (NonRetriableError)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add unique constraint for clause extractions</name>
  <files>db/schema/analyses.ts</files>
  <action>
Add unique constraint on clauseExtractions table for idempotent inserts.

1. **Import unique from drizzle-orm/pg-core** (add to existing imports):
```typescript
import {
  pgTable,
  text,
  uuid,
  integer,
  real,
  index,
  jsonb,
  timestamp,
  unique,  // ADD THIS
} from "drizzle-orm/pg-core"
```

2. **Add unique constraint to clauseExtractions table definition** (in the index array at the bottom of the table):
```typescript
// Add after existing indexes in the clauseExtractions table:
/**
 * Unique constraint for idempotent clause inserts.
 * Each chunk should produce at most one extraction per analysis.
 * Enables ON CONFLICT DO UPDATE for safe retries.
 */
unique("clause_analysis_chunk").on(table.analysisId, table.chunkId),
```

This constraint ensures that retrying the classifier step doesn't create duplicate clause extractions - the same (analysisId, chunkId) pair can only exist once.
  </action>
  <verify>
```bash
# Verify unique import was added
grep "unique" db/schema/analyses.ts | grep "import"
# Verify unique constraint was added
grep 'unique("clause_analysis_chunk")' db/schema/analyses.ts
# TypeScript compiles
cd /Users/medelman/GitHub/medelman17/vibedocs && pnpm exec tsc --noEmit db/schema/analyses.ts 2>&1 | head -10
```
  </verify>
  <done>clauseExtractions has unique constraint on (analysisId, chunkId) for idempotent inserts</done>
</task>

<task type="auto">
  <name>Task 2: Integrate validation gates and upserts into pipeline</name>
  <files>inngest/functions/analyze-nda.ts</files>
  <action>
Update analyze-nda.ts to integrate validation gates and upsert patterns.

**Note on scope:** Phase 1 focuses on validation gates for parser and classifier stages (the "entry gates"). These are critical because:
- If parser produces no text/chunks, downstream stages have nothing to work with
- If classifier produces 0 clauses, risk-scorer and gap-analyst have no input
Risk-scorer and gap-analyst validation is lower priority—if we have clauses, those stages have meaningful input.

1. **Add imports** (at top of file):
```typescript
import { validateParserOutput, validateClassifierOutput } from '@/agents/validation'
import { NonRetriableError } from '@/inngest/utils/errors'
import { eq } from 'drizzle-orm'
```

2. **Change analysis creation to deterministic ID pattern** (Step 1, around line 49):

The current code uses `insert().returning()` which creates new records on retry. Change to use a deterministic ID derived from event data:

```typescript
// Step 1: Create or update analysis record (idempotent)
// Deterministic ID: derived from documentId so retries always use same analysisId
// This works because one document has one active analysis at a time
const analysisId = crypto.createHash('sha256')
  .update(`analysis:${documentId}:${event.data.requestedAt ?? Date.now()}`)
  .digest('hex')
  .slice(0, 36)
  .replace(/(.{8})(.{4})(.{4})(.{4})(.{12})/, '$1-$2-$3-$4-$5')

await step.run('create-analysis', async () => {
  await ctx.db
    .insert(analyses)
    .values({
      id: analysisId,
      documentId,
      tenantId,
      status: 'processing',
      progressStage: 'parsing',
      progressPercent: 0,
    })
    .onConflictDoNothing()  // Safe: if ID exists, analysis already started
})
```

**Why deterministic ID:** `onConflictDoUpdate` with `target: analyses.id` doesn't work for initial insert—the conflict target must be a unique constraint that can match. By using a deterministic ID derived from event data (documentId + timestamp), retries produce the same ID, and `onConflictDoNothing` safely handles the "already exists" case.

3. **Add parser validation gate** (after parser step, around line 100):

```typescript
// Step 2: Parser Agent
const parserResult = await step.run('parser-agent', () =>
  runParserAgent({ documentId, tenantId, source, content, metadata })
)

// Parser validation gate - runs AFTER step completes, OUTSIDE step.run()
// Validation is fast and deterministic, so no durability needed
const parserValidation = validateParserOutput(
  parserResult.document.rawText,
  parserResult.document.chunks
)
if (!parserValidation.valid) {
  // Persist failure state (durable step for DB write)
  await step.run('mark-parser-failed', async () => {
    await ctx.db
      .update(analyses)
      .set({
        status: 'failed',
        progressStage: 'failed',
        metadata: {
          failedAt: 'parsing',
          errorCode: parserValidation.error!.code,
          errorMessage: parserValidation.error!.userMessage,
        },
      })
      .where(eq(analyses.id, analysisId))
  })
  // Throw non-retriable error with user-friendly message
  throw new NonRetriableError(parserValidation.error!.userMessage)
}

await emitProgress(
  'parsing',
  20,
  `Parsed ${parserResult.document.chunks.length} chunks`
)
```

4. **Add classifier validation gate** (after classifier step, around line 120):

```typescript
// Step 3: Classifier Agent
const classifierResult = await step.run('classifier-agent', () =>
  runClassifierAgent({
    parsedDocument: parserResult.document,
    budgetTracker,
  })
)

// Classifier validation gate - 0 clauses = always halt (per CONTEXT.md)
const classifierValidation = validateClassifierOutput(classifierResult.clauses)
if (!classifierValidation.valid) {
  await step.run('mark-classifier-failed', async () => {
    await ctx.db
      .update(analyses)
      .set({
        status: 'failed',
        progressStage: 'failed',
        metadata: {
          failedAt: 'classifying',
          errorCode: classifierValidation.error!.code,
          errorMessage: classifierValidation.error!.userMessage,
        },
      })
      .where(eq(analyses.id, analysisId))
  })
  throw new NonRetriableError(classifierValidation.error!.userMessage)
}

await emitProgress(
  'classifying',
  45,
  `Classified ${classifierResult.clauses.length} clauses`
)
```

5. **Important design notes**:
   - Validation runs OUTSIDE step.run() so failures trigger NonRetriableError immediately
   - Failure persistence runs INSIDE step.run() for durability—this is intentional
   - Trade-off: If validation passes but mark-failed step fails, we retry the mark-failed (acceptable)
  </action>
  <verify>
```bash
# Validation imports added
grep -q "validateParserOutput" inngest/functions/analyze-nda.ts
grep -q "validateClassifierOutput" inngest/functions/analyze-nda.ts
# NonRetriableError import
grep -q "NonRetriableError" inngest/functions/analyze-nda.ts
# eq import from drizzle-orm
grep "import.*eq.*from.*drizzle-orm" inngest/functions/analyze-nda.ts
# Deterministic ID pattern used
grep -q "createHash" inngest/functions/analyze-nda.ts
# TypeScript compiles
cd /Users/medelman/GitHub/medelman17/vibedocs && pnpm exec tsc --noEmit inngest/functions/analyze-nda.ts 2>&1 | head -20
```
  </verify>
  <done>analyze-nda.ts has validation gates after parser/classifier steps and uses deterministic ID for idempotent analysis creation</done>
</task>

<task type="auto">
  <name>Task 3: Push schema changes to database</name>
  <files>db/schema/analyses.ts</files>
  <action>
Run Drizzle schema push to apply the unique constraint to the database.

Execute:
```bash
cd /Users/medelman/GitHub/medelman17/vibedocs && pnpm db:push
```

This will add the `clause_analysis_chunk` unique constraint to the clauseExtractions table.

If the push fails due to existing duplicate data, the constraint may need to be added after cleaning up duplicates. Document any issues encountered.

Note: In local dev, `db:push` is safe. For production, this would need a proper migration.
  </action>
  <verify>
```bash
# Schema push completes (or document why it failed)
cd /Users/medelman/GitHub/medelman17/vibedocs && pnpm db:push 2>&1 | tail -20
```
  </verify>
  <done>Database schema updated with unique constraint (or documented blocker if existing data conflicts)</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Run `pnpm exec tsc --noEmit` to verify no type errors
2. Verify validation gates integrated: `grep -A5 "validateParserOutput" inngest/functions/analyze-nda.ts`
3. Run pipeline tests if they exist: `pnpm test inngest/functions/analyze-nda`
4. Verify unique constraint: `grep "clause_analysis_chunk" db/schema/analyses.ts`
</verification>

<success_criteria>
- analyze-nda.ts has validation gates after parser and classifier steps
- Validation failures trigger NonRetriableError with user-friendly message
- Analysis record set to status='failed' when validation fails
- Analysis creation uses deterministic ID pattern for idempotent inserts
- clauseExtractions has unique constraint on (analysisId, chunkId)
- All TypeScript compilation passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-hardening/01-03-SUMMARY.md`
</output>
