---
phase: 05-legal-chunking
plan: 03
type: execute
wave: 3
depends_on: ["05-01", "05-02"]
files_modified:
  - agents/parser.ts
  - inngest/functions/analyze-nda.ts
  - inngest/types.ts
autonomous: true

must_haves:
  truths:
    - "Parser agent extracts text and detects structure but no longer chunks or embeds"
    - "Chunking runs as a separate Inngest step after parsing"
    - "Embedding runs in batches of 128 with Voyage AI rate limiting between batches"
    - "Boilerplate chunks are stored with null embedding"
    - "Old chunks for the document are deleted before inserting new ones (replace strategy)"
    - "Chunk map and stats persisted to analyses table"
    - "Progress events include 'chunking' stage between parsing and classifying"
    - "Post-OCR pipeline also uses legal-aware chunking"
    - "Downstream agents receive chunks from DB, not from parser output"
  artifacts:
    - path: "agents/parser.ts"
      provides: "Text extraction + structure detection only (chunking removed)"
      exports: ["runParserAgent", "ParserOutput"]
    - path: "inngest/functions/analyze-nda.ts"
      provides: "Pipeline with separate chunk + embed + persist steps"
      contains: "chunkLegalDocument"
    - path: "inngest/types.ts"
      provides: "Updated progress stages including chunking"
      contains: "chunking"
  key_links:
    - from: "inngest/functions/analyze-nda.ts"
      to: "lib/document-chunking/legal-chunker.ts"
      via: "calls chunkLegalDocument in Inngest step"
      pattern: "chunkLegalDocument"
    - from: "inngest/functions/analyze-nda.ts"
      to: "lib/embeddings.ts"
      via: "calls embedBatch in batched Inngest steps"
      pattern: "embedBatch"
    - from: "inngest/functions/analyze-nda.ts"
      to: "db/schema/documents.ts"
      via: "bulk inserts chunks into documentChunks table"
      pattern: "documentChunks"
    - from: "inngest/functions/analyze-nda.ts"
      to: "db/schema/analyses.ts"
      via: "persists chunkMap and chunkStats"
      pattern: "chunkMap|chunkStats"
---

<objective>
Wire the legal chunker into the Inngest analysis pipeline, refactor the parser agent to separate extraction from chunking, and add batched embedding with database persistence.

Purpose: This connects all the infrastructure (Plan 01) and chunking logic (Plan 02) into the production pipeline. After this plan, uploading an NDA produces legal-aware chunks with Voyage AI embeddings stored in the tenant database, ready for downstream classification and risk scoring.

Output: Refactored parser agent, updated Inngest pipeline with separate chunk/embed/persist steps, updated event types.
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-legal-chunking/05-RESEARCH.md
@.planning/phases/05-legal-chunking/05-CONTEXT.md
@.planning/phases/05-legal-chunking/05-01-SUMMARY.md
@.planning/phases/05-legal-chunking/05-02-SUMMARY.md
@agents/parser.ts
@inngest/functions/analyze-nda.ts
@inngest/types.ts
@lib/embeddings.ts
@db/schema/documents.ts
@db/schema/analyses.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor parser agent to separate extraction from chunking</name>
  <files>agents/parser.ts, inngest/types.ts</files>
  <action>
**Refactor `agents/parser.ts`:**

The parser agent currently does THREE things: (1) extract text, (2) chunk, (3) embed. Refactor it to do ONLY extraction + structure detection. Chunking and embedding move to separate Inngest steps.

Changes:
1. Remove the import of `chunkDocument` from `@/lib/document-processing`
2. Remove the import of `getVoyageAIClient` from `@/lib/embeddings`
3. Remove `ParsedChunk` type (it's replaced by `LegalChunk` + `EmbeddedChunk` from the new module)

4. Update `ParserOutput` interface:
```typescript
export interface ParserOutput {
  document: {
    documentId: string
    title: string
    rawText: string
    structure: DocumentStructure
  }
  quality: {
    charCount: number
    wordCount: number
    pageCount: number
    confidence: number
    warnings: string[]
    isOcr?: boolean
  }
}
```
Key changes:
- Removed `chunks` from `document` (chunking happens later)
- Removed `tokenUsage` (embedding happens later)
- Added `isOcr` to quality

5. Update `runParserAgent()`:
- Remove the `chunkDocument()` call (lines ~186-187 in current code)
- Remove the `voyageClient.embedBatch()` call (lines ~189-192)
- Remove the chunk-embedding merge logic (lines ~194-197)
- Simply return `{ document: { documentId, title, rawText, structure }, quality }`
- For OCR source, set `quality.isOcr = true`
- Keep all the extraction logic (web, word-addin, OCR paths) exactly as-is

6. Remove the `DocumentChunk` import from `@/lib/document-processing` -- the parser no longer needs it.

**Update `inngest/types.ts`:**

Add `'chunking'` to the progress stage enum:

In `analysisProgressPayload`, update the `stage` enum:
```typescript
stage: z.enum([
  "parsing",
  "chunking",        // NEW - between parsing and classifying
  "ocr_processing",
  "classifying",
  "scoring",
  "analyzing_gaps",
  "complete",
  "failed",
]),
```

Also add chunk-related metadata fields to the progress metadata:
```typescript
metadata: z.object({
  chunksProcessed: z.number().int().nonnegative().optional(),
  totalChunks: z.number().int().nonnegative().optional(),
  clausesClassified: z.number().int().nonnegative().optional(),
  embeddingBatchesCompleted: z.number().int().nonnegative().optional(),
  totalEmbeddingBatches: z.number().int().nonnegative().optional(),
}).optional(),
```

IMPORTANT: Also update the `AnalysisProgressPayload` derived type -- it auto-derives from the schema, so no manual change needed.
  </action>
  <verify>
`npx tsc --noEmit` passes. The ParserOutput no longer includes chunks or embedding data. The 'chunking' stage is available in progress events.
  </verify>
  <done>
- Parser agent does extraction + structure detection only
- No more chunkDocument() or embedBatch() calls in parser
- ParserOutput.document no longer has chunks field
- 'chunking' added to progress stage enum in Inngest types
- OCR source sets quality.isOcr = true
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire chunking, embedding, and persistence into Inngest pipeline</name>
  <files>inngest/functions/analyze-nda.ts</files>
  <action>
**Update `inngest/functions/analyze-nda.ts`:**

This is the most critical task. The pipeline currently goes: parser -> validate -> classify -> score -> gaps. We insert chunking + embedding + persistence between parser and classifier.

Add imports at the top:
```typescript
import { chunkLegalDocument } from '@/lib/document-chunking/legal-chunker'
import { generateChunkMap, computeChunkStats } from '@/lib/document-chunking/chunk-map'
import { initVoyageTokenizer } from '@/lib/document-chunking/token-counter'
import { getVoyageAIClient, VOYAGE_CONFIG } from '@/lib/embeddings'
import { RATE_LIMITS } from '@/inngest/utils/rate-limit'
import { documentChunks } from '@/db/schema/documents'
import type { LegalChunk, EmbeddedChunk } from '@/lib/document-chunking/types'
```

**In `analyzeNda` function (main pipeline), after parser validation and budget validation:**

Remove the old chunk-based validation that references `parserResult.document.chunks` -- the parser no longer produces chunks. The budget validation currently uses parser chunks for token estimation. Replace this: estimate tokens directly from `rawText` length using a rough approximation (chars / 4), or use `countVoyageTokens()` on the rawText.

Actually, looking at the current code more carefully: `validateParserOutput` checks rawText and chunks. Since parser no longer produces chunks, update the call to `validateParserOutput` to only pass rawText (the function should still work -- it checks rawText is non-empty and chunks exist). Either:
- Update `validateParserOutput` to accept optional chunks (empty array is fine since chunking happens later), OR
- Simply pass an empty array for chunks to satisfy the existing validation gate (the "0 clauses" check will be moved to after chunking)

Recommended: Pass `[]` for chunks to `validateParserOutput`. The meaningful chunk validation will happen after the new chunking step.

Similarly, `validateTokenBudget` uses chunks. Adjust: pass rawText and an estimated chunk count based on text length, OR simplify to just validate raw text token count. The simplest fix: create a single fake chunk containing the full rawText for budget validation purposes. This preserves the existing budget gate behavior without changes.

**New pipeline steps after parser (insert between parser validation and classifier):**

**Step: Initialize tokenizer**
```typescript
await step.run('init-tokenizer', async () => {
  await initVoyageTokenizer()
})
```

**Step: Chunk document**
```typescript
const chunks = await step.run('chunk-document', async () => {
  return chunkLegalDocument(
    workingDocument.rawText,
    workingDocument.structure,
    { maxTokens: 512, targetTokens: 400, overlapTokens: 50, minChunkTokens: 50 }
  )
})
```

Note: `workingDocument` may have been truncated by budget validation. That's fine -- chunk the truncated version.

**Step: Generate and persist chunk map + stats**
```typescript
const { chunkMap, chunkStats } = await step.run('persist-chunk-metadata', async () => {
  const map = generateChunkMap(chunks, documentId)
  const stats = computeChunkStats(chunks)

  await ctx.db
    .update(analyses)
    .set({ chunkMap: map, chunkStats: stats })
    .where(eq(analyses.id, analysisId))

  return { chunkMap: map, chunkStats: stats }
})
```

**Emit chunking progress:**
```typescript
await emitProgress('chunking', 25, `Created ${chunks.length} legal chunks`)
```

**Steps: Embed chunks in batches with rate limiting**

Filter out boilerplate chunks (they get null embeddings):
```typescript
const embeddableChunks = chunks.filter(c => c.chunkType !== 'boilerplate')
const boilerplateChunks = chunks.filter(c => c.chunkType === 'boilerplate')
```

Batch embed (128 per batch, following existing `RATE_LIMITS.voyageAi.batchSize`):
```typescript
const voyageClient = getVoyageAIClient()
const batchSize = RATE_LIMITS.voyageAi.batchSize // 128
const embeddedChunks: EmbeddedChunk[] = []

// Process embeddable chunks in batches
for (let i = 0; i < embeddableChunks.length; i += batchSize) {
  const batchIndex = Math.floor(i / batchSize)
  const batch = embeddableChunks.slice(i, i + batchSize)
  const texts = batch.map(c => c.content)

  const result = await step.run(`embed-batch-${batchIndex}`, async () => {
    return voyageClient.embedBatch(texts, 'document')
  })

  for (let j = 0; j < batch.length; j++) {
    embeddedChunks.push({ ...batch[j], embedding: result.embeddings[j] })
  }

  // Rate limit between batches (skip after last batch)
  if (i + batchSize < embeddableChunks.length) {
    await step.sleep(`voyage-rate-limit-${batchIndex}`, getRateLimitDelay('voyageAi'))
  }
}

// Add boilerplate chunks with null embedding
for (const chunk of boilerplateChunks) {
  embeddedChunks.push({ ...chunk, embedding: null })
}

// Sort by index to maintain order
embeddedChunks.sort((a, b) => a.index - b.index)
```

**Step: Persist chunks to database**
```typescript
await step.run('persist-chunks', async () => {
  // Delete old chunks for this document (replace strategy for re-analysis)
  await ctx.db
    .delete(documentChunks)
    .where(eq(documentChunks.documentId, documentId))

  // Bulk insert in batches of 100 (DB connection limits)
  const DB_BATCH_SIZE = 100
  for (let i = 0; i < embeddedChunks.length; i += DB_BATCH_SIZE) {
    const batch = embeddedChunks.slice(i, i + DB_BATCH_SIZE)
    await ctx.db.insert(documentChunks).values(
      batch.map(chunk => ({
        tenantId,
        documentId,
        analysisId,
        chunkIndex: chunk.index,
        content: chunk.content,
        sectionPath: chunk.sectionPath,
        embedding: chunk.embedding,
        tokenCount: chunk.tokenCount,
        startPosition: chunk.startPosition,
        endPosition: chunk.endPosition,
        chunkType: chunk.chunkType,
        overlapTokens: chunk.metadata.overlapTokens,
        metadata: {
          references: chunk.metadata.references,
          parentClauseIntro: chunk.metadata.parentClauseIntro,
          structureSource: chunk.metadata.structureSource,
          isOcr: chunk.metadata.isOcr,
        },
      }))
    )
  }
})
```

**Emit embedding progress:**
```typescript
await emitProgress('chunking', 35, `Embedded and stored ${embeddedChunks.length} chunks`)
```

**Update downstream agent calls:**

The classifier agent currently receives `workingDocument` with chunks. Now it should receive chunks from the database or from the pipeline variable. The simplest approach: pass the `embeddedChunks` array to the classifier. Update the `runClassifierAgent` call to use the new chunk format.

HOWEVER, to minimize blast radius, keep the existing classifier/risk-scorer/gap-analyst interfaces unchanged for now. The classifier expects `parsedDocument` with chunks. Create a compatibility shim:

```typescript
// Create compatibility object for downstream agents
const parsedDocumentCompat = {
  ...workingDocument,
  chunks: embeddedChunks
    .filter(c => c.embedding !== null)  // Skip boilerplate for classification
    .map(c => ({
      id: c.id,
      index: c.index,
      content: c.content,
      sectionPath: c.sectionPath,
      tokenCount: c.tokenCount,
      startPosition: c.startPosition,
      endPosition: c.endPosition,
      embedding: c.embedding!,
    })),
}
```

Pass `parsedDocumentCompat` to `runClassifierAgent` instead of `workingDocument`.

**Also update `analyzeNdaAfterOcr` function (post-OCR pipeline):**

Apply the SAME changes to the post-OCR pipeline:
1. Parser no longer returns chunks -- just rawText + structure
2. Add the same chunking, embedding, persistence steps
3. Set `metadata.isOcr: true` on chunks (the parser quality will have `isOcr: true`)
4. Same compatibility shim for downstream agents

To avoid massive duplication, extract the chunking+embedding+persistence logic into a helper function:

```typescript
async function chunkEmbedPersist(
  ctx: TenantContext,
  step: InngestStep,
  rawText: string,
  structure: DocumentStructure,
  documentId: string,
  analysisId: string,
  tenantId: string,
  isOcr: boolean,
  emitProgress: (stage, progress, message) => Promise<void>
): Promise<EmbeddedChunk[]> {
  // ... all the chunking + embedding + persist logic from above
}
```

Call this helper from both `analyzeNda` and `analyzeNdaAfterOcr`.

Note on Inngest step naming: The step names within the helper need to be unique per invocation. Since each pipeline function has its own step namespace, this is fine -- just ensure step names don't collide within the same function.

**Update progress percentages:**
Adjust the progress flow to accommodate the new chunking stage:
- parsing: 10-15%
- chunking: 15-35% (chunking + embedding + persist)
- classifying: 35-55%
- scoring: 55-80%
- analyzing_gaps: 80-95%
- complete: 100%
  </action>
  <verify>
1. `npx tsc --noEmit` passes
2. `pnpm lint` passes (run `pnpm lint --fix` if needed)
3. Verify the pipeline flow: parser -> validate -> chunk -> embed -> persist -> classify -> score -> gaps
4. Verify both analyzeNda and analyzeNdaAfterOcr use the new chunking pipeline
5. Verify progress events include 'chunking' stage
6. Verify boilerplate chunks get null embedding
7. Verify old chunks are deleted before new ones are inserted (replace strategy)
  </verify>
  <done>
- Parser agent returns only text + structure (no chunks/embeddings)
- Chunking runs as separate Inngest step using chunkLegalDocument()
- Embedding runs in batches of 128 with rate limiting
- Boilerplate chunks stored with null embedding
- Old chunks deleted before re-insert (replace strategy)
- Chunk map and stats persisted to analyses table
- Both main and post-OCR pipelines use legal-aware chunking
- Progress events include 'chunking' stage at 15-35%
- Downstream agents receive compatible chunk format
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `pnpm lint` passes
3. `pnpm build` succeeds (verify no barrel export issues with new imports)
4. Pipeline flow is: parser -> validate -> init-tokenizer -> chunk-document -> persist-chunk-metadata -> embed-batch-N -> persist-chunks -> classify -> score -> gaps
5. Both analyzeNda and analyzeNdaAfterOcr use the same chunking pipeline
6. Progress events emit 'chunking' stage
7. No Inngest step explosion (batched embedding, single chunk persist step)
</verification>

<success_criteria>
- Full pipeline compiles and builds without errors
- Parser is simplified (extraction only)
- Legal-aware chunking replaces naive paragraph splitting
- Embeddings generated in batches with proper rate limiting
- Chunks persisted with all metadata (positions, type, references, section paths)
- Replace strategy for re-analysis (delete old, insert new)
- Chunk map and stats stored for monitoring
- Both web and OCR pipelines use legal chunking
- Progress tracking includes chunking stage
</success_criteria>

<output>
After completion, create `.planning/phases/05-legal-chunking/05-03-SUMMARY.md`
</output>
