---
phase: 05-legal-chunking
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - lib/document-chunking/legal-chunker.ts
  - lib/document-chunking/chunk-strategies.ts
  - lib/document-chunking/chunk-merger.ts
  - lib/document-chunking/cross-reference.ts
  - lib/document-chunking/chunk-map.ts
autonomous: true

must_haves:
  truths:
    - "Definitions sections produce one chunk per definition term"
    - "Lettered sub-clauses (a), (b), (c) each become their own chunk"
    - "Chunks respect section boundaries (no split mid-clause)"
    - "No chunk exceeds 512 tokens (verified by Llama 2 tokenizer)"
    - "Short chunks under 50 tokens are merged with adjacent siblings"
    - "Cross-references annotated in metadata (e.g., references: ['3.1', '7.4'])"
    - "Chunk map summary generated per document with stats and entries"
    - "Boilerplate/signature chunks marked with chunkType 'boilerplate'"
    - "Unstructured documents fall back to paragraph-based splitting"
  artifacts:
    - path: "lib/document-chunking/legal-chunker.ts"
      provides: "Main entry point chunkLegalDocument()"
      exports: ["chunkLegalDocument"]
    - path: "lib/document-chunking/chunk-strategies.ts"
      provides: "Strategy implementations for definitions, clauses, sub-clauses, recitals, boilerplate, exhibits, fallback"
      exports: ["chunkDefinitions", "chunkClause", "chunkBoilerplate", "chunkExhibit", "chunkRecital", "chunkFallback"]
    - path: "lib/document-chunking/chunk-merger.ts"
      provides: "Short chunk merging and oversized chunk splitting"
      exports: ["mergeShortChunks", "splitOversizedChunks"]
    - path: "lib/document-chunking/cross-reference.ts"
      provides: "Cross-reference extraction from legal text"
      exports: ["extractCrossReferences"]
    - path: "lib/document-chunking/chunk-map.ts"
      provides: "Chunk map summary generator"
      exports: ["generateChunkMap", "computeChunkStats"]
  key_links:
    - from: "lib/document-chunking/legal-chunker.ts"
      to: "lib/document-chunking/chunk-strategies.ts"
      via: "calls strategy functions based on section type"
      pattern: "chunkDefinitions|chunkClause|chunkBoilerplate"
    - from: "lib/document-chunking/legal-chunker.ts"
      to: "lib/document-chunking/chunk-merger.ts"
      via: "post-processes chunks for size compliance"
      pattern: "mergeShortChunks|splitOversizedChunks"
    - from: "lib/document-chunking/legal-chunker.ts"
      to: "lib/document-chunking/token-counter.ts"
      via: "counts tokens using Voyage AI tokenizer"
      pattern: "countVoyageTokensSync|countVoyageTokens"
    - from: "lib/document-chunking/chunk-strategies.ts"
      to: "lib/document-extraction/types.ts"
      via: "consumes PositionedSection from structure detector"
      pattern: "PositionedSection"
---

<objective>
Build the legal-aware document chunking engine that transforms extracted text + DocumentStructure into right-sized, metadata-rich chunks respecting legal document boundaries.

Purpose: This is the core algorithmic work of Phase 5. It replaces the simplistic paragraph-based `chunkDocument()` with structure-aware chunking that produces granular, well-typed chunks suitable for RAG retrieval, classification, and risk scoring.

Output: Five modules implementing the full chunking pipeline: main entry, strategies, merging, cross-references, and chunk map generation.
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-legal-chunking/05-RESEARCH.md
@.planning/phases/05-legal-chunking/05-CONTEXT.md
@.planning/phases/05-legal-chunking/05-01-SUMMARY.md
@lib/document-extraction/types.ts
@lib/document-extraction/structure-detector.ts
@lib/document-processing.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create legal chunker entry point and chunking strategies</name>
  <files>lib/document-chunking/legal-chunker.ts, lib/document-chunking/chunk-strategies.ts</files>
  <action>
**Create `lib/document-chunking/legal-chunker.ts`:**

This is the main entry point that replaces `chunkDocument()`. It consumes `DocumentStructure` from Phase 3 and produces `LegalChunk[]`.

```typescript
export async function chunkLegalDocument(
  text: string,
  structure: DocumentStructure,
  options?: Partial<LegalChunkOptions>
): Promise<LegalChunk[]>
```

Implementation flow:
1. Initialize Voyage AI tokenizer (call `initVoyageTokenizer()` once)
2. Merge options with defaults: `{ maxTokens: 512, targetTokens: 400, overlapTokens: 50, minChunkTokens: 50, skipBoilerplateEmbedding: true }`
3. Determine `structureSource`: check if structure came from regex or LLM. If `structure.sections.length === 0`, this is unstructured -- use fallback strategy on the entire text.
4. Iterate over `structure.sections` and dispatch to appropriate strategy based on `section.type`:
   - `'definitions'` -> `chunkDefinitions(section, text, options)`
   - `'clause'` or `'heading'` -> `chunkClause(section, text, options)`
   - `'signature'` -> `chunkBoilerplate(section, text, options)`
   - `'exhibit'` -> `chunkExhibit(section, text, options)`
   - `'cover_letter'` -> `chunkBoilerplate(section, text, options)`
   - `'schedule'` -> `chunkExhibit(section, text, options)` (treat like exhibit)
   - `'amendment'` -> `chunkClause(section, text, options)` (treat like clause)
   - `'other'` -> `chunkClause(section, text, options)`
5. Handle text between detected sections (gaps): if there is text between `section[i].endOffset` and `section[i+1].startOffset`, chunk it as `chunkFallback()`.
6. Also handle text before first section and after last section as fallback chunks.
7. Post-process: call `mergeShortChunks()` then `splitOversizedChunks()` (import from chunk-merger.ts)
8. Annotate cross-references on all chunks (call `extractCrossReferences` for each chunk content, store in metadata.references)
9. Add overlap: for each chunk (except the first), prepend up to `overlapTokens` tokens from the end of the previous chunk. Set `metadata.isOverlap = true` and `metadata.overlapTokens = N`. IMPORTANT: the overlap text is prepended to `content` for embedding quality, but `startPosition`/`endPosition` track the ORIGINAL text positions (not including overlap).
10. Re-index all chunks sequentially (index = 0, 1, 2...) and assign `id = 'chunk-{index}'`
11. Return the final `LegalChunk[]`

Handle the edge case where `structure.sections` is empty (completely unstructured document): call `chunkFallback()` on the full text. Per CONTEXT.md, if chunks/page ratio is < 2, log a warning suggesting LLM re-chunking may be needed (but don't actually trigger it in this phase -- that's a future enhancement).

**Create `lib/document-chunking/chunk-strategies.ts`:**

Import `PositionedSection` from `@/lib/document-extraction/types` and `LegalChunk`, `ChunkType`, `LegalChunkOptions` from `./types`. Import `countVoyageTokensSync` from `./token-counter`.

**`chunkDefinitions(section, fullText, options)`:**
- Extract content: `fullText.slice(section.startOffset, section.endOffset)`
- Split on definition patterns: `/"([^"]+)"\s+(?:means|shall mean|refers to|has the meaning)/gi` and also smart quotes variants
- Each match becomes its own chunk with `chunkType: 'definition'`, `sectionPath: [...section.sectionPath, 'Definition: {term}']`
- If a definition exceeds maxTokens, it stays as-is (will be split in post-processing)
- Any text in the definitions section that doesn't match a definition pattern becomes a clause chunk (the section intro, e.g., "The following terms shall have the meanings...")
- Positions: `startPosition = section.startOffset + match.index`, `endPosition = startPosition + match[0].length`

**`chunkClause(section, fullText, options)`:**
- Extract content: `fullText.slice(section.startOffset, section.endOffset)`
- Check for lettered sub-clauses: pattern `\n\s*\(([a-z])\)\s+` or `\n\s*\(([ivx]+)\)\s+` (Roman numerals too)
- If sub-clauses found:
  - Extract intro text (content before first sub-clause marker)
  - If intro is non-trivial (> 20 chars), create a clause chunk for it
  - Each sub-clause becomes its own chunk with:
    - `chunkType: 'sub-clause'`
    - `sectionPath: [...section.sectionPath, '({letter})']`
    - `metadata.parentClauseIntro`: first 100 tokens (using countVoyageTokensSync to measure) of the section content, truncated at word boundary
  - Calculate positions relative to section.startOffset
- If no sub-clauses: create a single chunk with `chunkType: 'clause'`, `sectionPath: section.sectionPath`
- All positions track into original `fullText`

**`chunkBoilerplate(section, fullText, options)`:**
- Create chunk(s) with `chunkType: 'boilerplate'`
- If content exceeds maxTokens, split at paragraph boundaries
- Section path from section.sectionPath

**`chunkExhibit(section, fullText, options)`:**
- Create chunk(s) with `chunkType: 'exhibit'`
- If substantive (> 100 tokens), chunk normally like clauses
- If short, create single chunk

**`chunkRecital(section, fullText, options)`:**
- Check for WHEREAS/recital patterns in the section
- Each "WHEREAS" paragraph becomes its own chunk with `chunkType: 'recital'`
- If no WHEREAS pattern, chunk as single recital

**`chunkFallback(text, startOffset, options, sectionPath?)`:**
- For unstructured text or gaps between sections
- Split on paragraph boundaries (double newline)
- Each paragraph becomes a chunk with `chunkType: 'fallback'`
- If a paragraph exceeds maxTokens, split at sentence boundaries (period + space)
- Track positions relative to startOffset

All strategy functions return `LegalChunk[]`. They use `countVoyageTokensSync()` for token counting (the tokenizer will already be initialized by the time strategies run).

Set `metadata.isOverlap = false` and `metadata.overlapTokens = 0` as defaults in all strategy functions (overlap is added later in post-processing by the main chunker).
  </action>
  <verify>
`npx tsc --noEmit` passes. Manually inspect that:
- chunkLegalDocument is exported and accepts (text, structure, options?)
- All strategy functions handle edge cases (empty sections, no matches)
- Positions reference the original fullText, not sliced content
  </verify>
  <done>
- chunkLegalDocument() exported from legal-chunker.ts
- Six strategy functions exported from chunk-strategies.ts: chunkDefinitions, chunkClause, chunkBoilerplate, chunkExhibit, chunkRecital, chunkFallback
- Definitions split into one chunk per term
- Sub-clauses (a), (b), (c) each become their own chunk
- All chunks have correct chunkType, sectionPath, positions, and metadata
- Unstructured text falls back to paragraph-based splitting
  </done>
</task>

<task type="auto">
  <name>Task 2: Create post-processing modules (merger, cross-reference, chunk-map)</name>
  <files>lib/document-chunking/chunk-merger.ts, lib/document-chunking/cross-reference.ts, lib/document-chunking/chunk-map.ts</files>
  <action>
**Create `lib/document-chunking/chunk-merger.ts`:**

Two exported functions:

**`mergeShortChunks(chunks: LegalChunk[], minTokens: number): LegalChunk[]`**
- Iterate through chunks. If a chunk has `tokenCount < minTokens` (default 50):
  - Try to merge with the NEXT sibling chunk (same parent section path, adjacent index)
  - If next chunk also under target tokens after merge, do the merge
  - Merged chunk: combine content with `\n\n` separator, use earlier startPosition and later endPosition, set `chunkType: 'merged'`, combine sectionPaths (use the more specific one), sum tokenCounts
  - If no suitable merge candidate (e.g., last chunk, or next chunk is a different type), leave as-is
- Do NOT merge across different section paths at the top level (e.g., don't merge an Article 3 chunk with an Article 4 chunk)
- Do NOT merge chunks of type 'boilerplate' with non-boilerplate chunks

**`splitOversizedChunks(chunks: LegalChunk[], maxTokens: number): LegalChunk[]`**
- Iterate through chunks. If `tokenCount > maxTokens`:
  - Split at sentence boundaries (regex: `/(?<=[.!?])\s+/`)
  - Build sub-chunks by accumulating sentences until approaching maxTokens
  - Each sub-chunk inherits the parent's sectionPath and gets `chunkType: 'split'`
  - Positions: calculate from the parent chunk's startPosition using indexOf to find each sentence's offset
  - If no sentence boundaries found (one giant sentence), fall back to splitting at word boundaries at the maxTokens limit
- Re-count tokens using `countVoyageTokensSync()` for each resulting sub-chunk

**Create `lib/document-chunking/cross-reference.ts`:**

Single exported function:

**`extractCrossReferences(text: string): string[]`**
- Apply these regex patterns to extract section references:
  - `Section\s+(\d+(?:\.\d+)*)`
  - `Article\s+([IVX\d]+)`
  - `(?:paragraph|clause)\s+(\d+(?:\.\d+)*(?:\([a-z]\))?)`
  - `(?:as defined in|pursuant to|in accordance with|subject to)\s+Section\s+(\d+(?:\.\d+)*)`
- Deduplicate using a Set
- Return sorted array of unique references
- Handle edge case: don't extract self-references (would need chunk's own section path, but for simplicity, extract all and let downstream consumers filter)

**Create `lib/document-chunking/chunk-map.ts`:**

Two exported functions:

**`generateChunkMap(chunks: LegalChunk[], documentId: string): ChunkMap`**
- Compute:
  - `totalChunks`: chunks.length
  - `avgTokens`: Math.round(sum / count)
  - `minTokens`: Math.min(...)
  - `maxTokens`: Math.max(...)
  - `distribution`: count per chunkType
  - `entries`: array of ChunkMapEntry with index, sectionPath, type, tokenCount, preview (first 100 chars)
- Return ChunkMap object

**`computeChunkStats(chunks: LegalChunk[]): ChunkStats`**
- Same stats as chunk map but without entries list (lightweight for DB storage):
  - totalChunks, avgTokens, minTokens, maxTokens, distribution
- Return ChunkStats object (matches the type from types.ts)

Handle edge cases:
- Empty chunks array: return zeroed stats
- Single chunk: min = max = avg
  </action>
  <verify>
`npx tsc --noEmit` passes. Verify all five files in `lib/document-chunking/` compile cleanly. Ensure mergeShortChunks and splitOversizedChunks maintain position accuracy.
  </verify>
  <done>
- mergeShortChunks() merges chunks under 50 tokens with adjacent siblings
- splitOversizedChunks() splits chunks over 512 tokens at sentence boundaries
- extractCrossReferences() detects Section/Article/clause references
- generateChunkMap() produces complete chunk map with entries
- computeChunkStats() produces lightweight stats for DB storage
- All functions handle edge cases (empty input, single item)
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors across all 5 new files
2. `chunkLegalDocument()` can be called with a DocumentStructure and text string
3. All strategy functions handle their respective section types
4. Post-processing correctly merges short chunks and splits oversized ones
5. Cross-references are extracted from legal text patterns
6. Chunk map produces valid summary with stats
</verification>

<success_criteria>
- chunkLegalDocument() is the single entry point for legal-aware chunking
- Definitions produce one chunk per defined term
- Sub-clauses (a), (b), (c) produce one chunk each with parentClauseIntro metadata
- No chunk exceeds 512 Voyage AI tokens after post-processing
- Short chunks under 50 tokens are merged with adjacent siblings
- Cross-references annotated in chunk metadata
- Chunk map and stats generated for monitoring
- Boilerplate/signature sections chunked but marked appropriately
- Unstructured text falls back to paragraph-based splitting
</success_criteria>

<output>
After completion, create `.planning/phases/05-legal-chunking/05-02-SUMMARY.md`
</output>
