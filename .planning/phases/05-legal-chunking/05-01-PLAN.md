---
phase: 05-legal-chunking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/document-chunking/types.ts
  - lib/document-chunking/token-counter.ts
  - db/schema/documents.ts
  - db/schema/analyses.ts
autonomous: true

must_haves:
  truths:
    - "LegalChunk type captures content, sectionPath, positions, chunkType, and metadata"
    - "Token counter uses Llama 2 tokenizer matching Voyage AI voyage-law-2"
    - "documentChunks table has startPosition, endPosition, chunkType, analysisId, overlapTokens columns"
    - "analyses table has chunkMap and chunkStats JSONB columns"
  artifacts:
    - path: "lib/document-chunking/types.ts"
      provides: "LegalChunk, ChunkMetadata, ChunkStats, ChunkType, LegalChunkOptions types"
      exports: ["LegalChunk", "ChunkMetadata", "ChunkStats", "ChunkType", "ChunkMapEntry", "ChunkMap", "LegalChunkOptions"]
    - path: "lib/document-chunking/token-counter.ts"
      provides: "Voyage AI token counting via Llama 2 tokenizer"
      exports: ["countVoyageTokens", "countVoyageTokensSync", "initVoyageTokenizer"]
    - path: "db/schema/documents.ts"
      provides: "Extended documentChunks table with position tracking and chunk type"
      contains: "startPosition"
    - path: "db/schema/analyses.ts"
      provides: "chunkMap and chunkStats fields on analyses"
      contains: "chunkMap"
  key_links:
    - from: "lib/document-chunking/types.ts"
      to: "lib/document-extraction/types.ts"
      via: "imports PositionedSection, DocumentStructure"
      pattern: "import.*from.*document-extraction/types"
    - from: "lib/document-chunking/token-counter.ts"
      to: "llama-tokenizer-js"
      via: "dynamic import for lazy singleton"
      pattern: "import\\('llama-tokenizer-js'\\)"
---

<objective>
Create the foundation infrastructure for legal-aware document chunking: TypeScript types, Voyage AI token counter using Llama 2 tokenizer, and database schema extensions.

Purpose: All subsequent chunking logic and pipeline integration depends on these types, the accurate token counter, and the extended database schema. This is the leaf dependency for the entire phase.

Output: Types module, token counter module, extended documentChunks and analyses schemas.
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-legal-chunking/05-RESEARCH.md
@.planning/phases/05-legal-chunking/05-CONTEXT.md
@lib/document-extraction/types.ts
@lib/document-processing.ts
@db/schema/documents.ts
@db/schema/analyses.ts
@db/_columns.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create types module and token counter with llama-tokenizer-js</name>
  <files>lib/document-chunking/types.ts, lib/document-chunking/token-counter.ts</files>
  <action>
First, install the dependency:
```
pnpm add llama-tokenizer-js
```

Create `lib/document-chunking/types.ts` with these types:

```typescript
// Import PositionedSection and DocumentStructure from document-extraction/types
import type { PositionedSection, DocumentStructure } from '@/lib/document-extraction/types'
```

**LegalChunk interface:**
- `id: string` - chunk identifier (format: `chunk-{index}`)
- `index: number` - sequential ordering (0-based) for document reconstruction
- `content: string` - the chunk text
- `sectionPath: string[]` - hierarchical path, e.g. `["Article 5", "Section 5.2", "(a)"]`
- `tokenCount: number` - Voyage AI token count (Llama 2 tokenizer)
- `startPosition: number` - character offset in original extracted text
- `endPosition: number` - character offset end (exclusive)
- `chunkType: ChunkType` - type discriminator
- `metadata: ChunkMetadata` - additional metadata

**ChunkType union:**
`'definition' | 'clause' | 'sub-clause' | 'recital' | 'boilerplate' | 'exhibit' | 'merged' | 'split' | 'fallback'`

**ChunkMetadata interface:**
- `parentClauseIntro?: string` - first ~100 tokens of parent clause text (for sub-chunks)
- `references: string[]` - cross-references: `["3.1", "7.4"]`
- `isOverlap: boolean` - whether this chunk has overlap text prepended
- `overlapTokens: number` - how many tokens are overlap
- `structureSource: 'regex' | 'llm'` - how the document structure was detected
- `isOcr?: boolean` - true if text came from OCR processing

**ChunkStats interface:**
- `totalChunks: number`
- `avgTokens: number`
- `minTokens: number`
- `maxTokens: number`
- `distribution: Record<ChunkType, number>` - count per type

**ChunkMapEntry interface:**
- `index: number`
- `sectionPath: string[]`
- `type: ChunkType`
- `tokenCount: number`
- `preview: string` - first 100 chars

**ChunkMap interface:**
- `documentId: string`
- `totalChunks: number`
- `avgTokens: number`
- `minTokens: number`
- `maxTokens: number`
- `distribution: Record<string, number>`
- `entries: ChunkMapEntry[]`

**LegalChunkOptions interface:**
- `maxTokens?: number` - hard max per chunk (default 512)
- `targetTokens?: number` - soft target (default 400)
- `overlapTokens?: number` - overlap between chunks (default 50)
- `minChunkTokens?: number` - merge threshold (default 50)
- `skipBoilerplateEmbedding?: boolean` - skip embedding for boilerplate (default true)

**EmbeddedChunk type:** `LegalChunk & { embedding: number[] | null }` (null for skipped boilerplate)

Create `lib/document-chunking/token-counter.ts`:

Use the **lazy singleton pattern** with dynamic import to avoid barrel export issues (per CLAUDE.md conventions and RESEARCH.md Pitfall 7).

Per Voyage AI official documentation (confirmed in RESEARCH.md), voyage-law-2 uses the Llama 2 tokenizer. `llama-tokenizer-js` is the JavaScript implementation of this same SentencePiece tokenizer, making it the accurate choice for Voyage AI token counting. This satisfies the CONTEXT.md decision to "match Voyage AI's tokenizer." Using `gpt-tokenizer` (tiktoken-based) would undercount by 10-20% because Voyage AI's Llama 2 tokenizer produces more tokens than OpenAI's tiktoken on legal text.

```typescript
let _llamaTokenizer: { encode: (text: string) => number[] } | null = null

async function getLlamaTokenizer() {
  if (!_llamaTokenizer) {
    const mod = await import('llama-tokenizer-js')
    _llamaTokenizer = mod.default
  }
  return _llamaTokenizer
}
```

Export three functions:
1. `initVoyageTokenizer(): Promise<void>` - pre-warms the tokenizer (call once at pipeline start)
2. `countVoyageTokens(text: string): Promise<number>` - async, always accurate
3. `countVoyageTokensSync(text: string): number` - sync, falls back to `Math.ceil(text.length / 4.5)` if tokenizer not yet loaded

Add JSDoc on each function explaining that voyage-law-2 uses the Llama 2 tokenizer per Voyage AI official docs (confirmed in RESEARCH.md Sources section, Primary #1), and that `llama-tokenizer-js` IS the JS implementation of the Llama 2 SentencePiece tokenizer. Note that `gpt-tokenizer` undercounts by 10-20% and must not be used for Voyage AI token counting.
  </action>
  <verify>
Run `pnpm build` (or at minimum `npx tsc --noEmit`) to verify types compile without errors. Verify llama-tokenizer-js is in package.json dependencies.
  </verify>
  <done>
- LegalChunk, ChunkType, ChunkMetadata, ChunkStats, ChunkMap, LegalChunkOptions, EmbeddedChunk types exported from types.ts
- countVoyageTokens, countVoyageTokensSync, initVoyageTokenizer exported from token-counter.ts
- llama-tokenizer-js installed in package.json
- JSDoc on token-counter.ts explicitly documents voyage-law-2 = Llama 2 tokenizer per Voyage AI docs, and that llama-tokenizer-js is the JS implementation of this tokenizer
- No TypeScript errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend documentChunks and analyses schemas</name>
  <files>db/schema/documents.ts, db/schema/analyses.ts</files>
  <action>
**Extend `db/schema/documents.ts` - documentChunks table:**

Add these columns to the existing `documentChunks` table definition:

1. `startPosition: integer("start_position")` - character offset in original text
2. `endPosition: integer("end_position")` - character offset end (exclusive)
3. `chunkType: text("chunk_type")` - one of the ChunkType values ('definition', 'clause', 'sub-clause', etc.)
4. `analysisId: uuid("analysis_id")` - link to specific analysis run. Store as a plain `uuid` column WITHOUT `.references()`. Do NOT add a foreign key reference to analyses. The application layer enforces this relationship. This avoids circular imports since `db/schema/analyses.ts` already imports from `db/schema/documents.ts`.
5. `overlapTokens: integer("overlap_tokens").default(0)` - how many tokens are overlap from previous chunk

Also update the unique constraint. The current unique constraint is `(documentId, chunkIndex)`. Since chunks are now per-analysis (re-analysis creates new chunks), change the unique constraint to include analysisId: `unique("chunk_doc_analysis_index").on(table.documentId, table.analysisId, table.chunkIndex)`. Remove or keep the old `chunk_doc_index` constraint as needed -- since the old constraint is named, Drizzle will generate a migration that drops it. Add the new one alongside.

Add an index: `index("idx_chunks_analysis").on(table.analysisId)` for efficient analysis-scoped chunk queries.

**Extend `db/schema/analyses.ts` - analyses table:**

Add two new JSONB columns:

1. `chunkMap: jsonb("chunk_map")` - summary of all chunks for debugging (ChunkMap type)
2. `chunkStats: jsonb("chunk_stats")` - statistics { total, avg, min, max, distribution }

Add a `'chunking'` value to the progressStage comments to document the new stage.

After making schema changes, run:
```bash
pnpm db:generate
```

This generates the migration SQL. Do NOT run `pnpm db:push` (that's a production concern).

Note: Do NOT add `import 'server-only'` to schema files -- they need to be importable from test files.
  </action>
  <verify>
Run `npx tsc --noEmit` to verify no circular import issues or type errors. Run `pnpm db:generate` and check that a migration file is created in `./drizzle/` with the expected ALTER TABLE statements.
  </verify>
  <done>
- documentChunks has startPosition, endPosition, chunkType, analysisId (plain uuid, no FK reference), overlapTokens columns
- analyses has chunkMap and chunkStats JSONB columns
- Unique constraint updated to include analysisId
- idx_chunks_analysis index added
- Migration generated successfully
- No circular import issues
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `pnpm db:generate` produces a valid migration
3. `lib/document-chunking/types.ts` exports all required types
4. `lib/document-chunking/token-counter.ts` exports countVoyageTokens, countVoyageTokensSync, initVoyageTokenizer
5. llama-tokenizer-js is in package.json dependencies
</verification>

<success_criteria>
- All types compile and are importable from `@/lib/document-chunking/types`
- Token counter uses lazy dynamic import of llama-tokenizer-js (no barrel export issues)
- Token counter JSDoc explicitly documents the Voyage AI -> Llama 2 -> llama-tokenizer-js chain with source reference
- Schema extensions compile and generate valid migrations
- No circular dependency between documents.ts and analyses.ts (analysisId is plain uuid without FK reference)
</success_criteria>

<output>
After completion, create `.planning/phases/05-legal-chunking/05-01-SUMMARY.md`
</output>
