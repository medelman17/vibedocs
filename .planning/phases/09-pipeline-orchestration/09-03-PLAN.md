---
phase: 09-pipeline-orchestration
plan: 03
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - inngest/functions/analyze-nda.ts
autonomous: true

must_haves:
  truths:
    - "Classifier runs as per-batch Inngest steps, not a single monolithic step"
    - "Chunk-level progress visible: 'Classifying clause 7 of 15...'"
    - "Each classifier batch is independently retriable on failure"
    - "Rate limit delays between classifier batches for Claude"
  artifacts:
    - path: "inngest/functions/analyze-nda.ts"
      provides: "Per-batch classifier steps with chunk-level progress"
      contains: "classify-batch-"
  key_links:
    - from: "inngest/functions/analyze-nda.ts"
      to: "agents/classifier.ts"
      via: "Per-batch calls to classifier with subset of chunks"
      pattern: "classify-batch"
---

<objective>
Split classifier into per-batch Inngest steps for chunk-level progress and independent retry.

Purpose: Currently the classifier runs as a single monolithic step.run() call. This prevents showing "Classifying clause 7 of 15..." progress and means a failure 90% through the step loses all work. Splitting into per-batch steps gives granular progress updates and makes each batch independently retriable.
Output: analyze-nda.ts with per-batch classifier steps and chunk-level progress emissions between batches.
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-pipeline-orchestration/09-RESEARCH.md
@.planning/phases/09-pipeline-orchestration/09-01-PLAN.md
@inngest/functions/analyze-nda.ts
@agents/classifier.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Split classifier into per-batch Inngest steps</name>
  <files>inngest/functions/analyze-nda.ts</files>
  <action>
  The classifier agent currently processes all chunks internally in batches (CLASSIFIER_BATCH_SIZE = 4), but the entire operation runs in a single `step.run('classifier-agent', ...)`. This needs to be refactored so each batch is a separate Inngest step.

  **Approach:** Instead of calling `runClassifierAgent()` which handles batching internally, we need to call the classifier's per-batch processing directly. However, since `runClassifierAgent` does its own batching logic with RAG retrieval and neighbor context, the cleanest approach is to:

  1. Keep `runClassifierAgent` as-is (it returns `clauses` and `rawClassifications`)
  2. BUT wrap it in a way that its internal batches become separate Inngest steps

  The simplest and most practical approach: Since `runClassifierAgent` already processes batches of 4 chunks internally, and a typical NDA has 15-30 chunks, the entire classifier call takes maybe 15-45 seconds. Rather than refactoring the agent internals, we can:

  a) Keep the single `classifier-agent` step for the actual LLM work
  b) Add progress emissions AROUND it with estimated sub-progress

  Actually, re-reading the CONTEXT.md decisions: "Chunk-level progress visible in long stages (Scoring clause 7 of 15...)" is a requirement. To deliver real chunk-level progress, we need to pass the step function into the agent or split at the Inngest level.

  **Better approach: Create a `runClassifierBatch` wrapper function** that processes a single batch of chunks. Then in the pipeline, iterate batches at the Inngest step level:

  Import the necessary internals from the classifier module. If `runClassifierAgent` isn't batch-splittable (it handles RAG retrieval, neighbor context, etc.), create a new exported function `classifyChunkBatch` in `agents/classifier.ts` that processes a single batch.

  However, per the plan constraints (2-3 files max and focused scope), the pragmatic approach is:

  **In analyze-nda.ts, refactor the classifier section in BOTH analyzeNda and analyzeNdaAfterOcr:**

  Replace the single `step.run('classifier-agent', ...)` with a batched loop:

  ```typescript
  // Step 8: Classifier Agent (per-batch for chunk-level progress)
  const CLASSIFIER_BATCH_SIZE = 4 // Match classifier internal batch size
  const classifierChunks = classifierDocument.chunks
  const totalClassifierBatches = Math.ceil(classifierChunks.length / CLASSIFIER_BATCH_SIZE)

  let allClassifications: typeof classifierResult.rawClassifications = []
  let allClauses: typeof classifierResult.clauses = []

  for (let batch = 0; batch < totalClassifierBatches; batch++) {
    const batchStart = batch * CLASSIFIER_BATCH_SIZE
    const batchEnd = Math.min(batchStart + CLASSIFIER_BATCH_SIZE, classifierChunks.length)
    const batchChunks = classifierChunks.slice(batchStart, batchEnd)

    // Create a sub-document for this batch
    const batchDocument = {
      ...classifierDocument,
      chunks: batchChunks,
    }

    const batchResult = await step.run(`classify-batch-${batch}`, () =>
      runClassifierAgent({
        parsedDocument: batchDocument,
        budgetTracker,
      })
    )

    allClassifications.push(...batchResult.rawClassifications)
    allClauses.push(...batchResult.clauses)

    // Chunk-level progress
    const processed = Math.min(batchEnd, classifierChunks.length)
    await emitProgress(
      'classifying',
      40 + Math.round((processed / classifierChunks.length) * 20), // 40-60% range
      `Classifying clause ${processed} of ${classifierChunks.length}...`
    )

    // Rate limit between batches (Claude 60 RPM)
    if (batch < totalClassifierBatches - 1) {
      await step.sleep(`rate-limit-classify-${batch}`, getRateLimitDelay('claude'))
    }
  }

  // Reassemble the combined result for downstream use
  const classifierResult = {
    clauses: allClauses,
    rawClassifications: allClassifications,
  }
  ```

  **Important type considerations:**
  - `runClassifierAgent` returns `{ clauses, rawClassifications }`. When processing batches, chunk indices in `rawClassifications` are relative to the batch's `parsedDocument.chunks`, but `chunkIndex` in rawClassifications corresponds to the global `chunk.index` property which is preserved since we're slicing the original chunks array.
  - The classifier uses neighbor context (chunks before/after) -- by passing the full document but only a slice of chunks, the classifier still has access to neighbor text via the rawText. However, it may lose the neighbor context optimization. This is acceptable for now since the main purpose is progress granularity and retriability.

  Remove the old single-step classifier call and its emitProgress call that came after.

  Apply the same refactoring to the `analyzeNdaAfterOcr` function.
  </action>
  <verify>Run `pnpm lint`. Grep for `classify-batch-` to confirm per-batch step IDs exist. Verify no remaining `step.run('classifier-agent'` (old monolithic step).</verify>
  <done>Classifier runs as per-batch Inngest steps with chunk-level progress messages like "Classifying clause 7 of 15...", each batch independently retriable, rate limits between batches.</done>
</task>


</tasks>

<verification>
1. `pnpm lint` passes
2. Grep for `classify-batch-` in analyze-nda.ts returns matches for both functions
3. No remaining `step.run('classifier-agent'` (old monolithic step)
4. emitProgress calls show clause-level messages with counts
</verification>

<success_criteria>
- Classifier uses per-batch steps: `classify-batch-0`, `classify-batch-1`, etc.
- Progress messages show "Classifying clause X of Y..."
- Rate limit sleeps between classifier batches
- Both analyzeNda and analyzeNdaAfterOcr updated consistently
</success_criteria>

<output>
After completion, create `.planning/phases/09-pipeline-orchestration/09-03-SUMMARY.md`
</output>
