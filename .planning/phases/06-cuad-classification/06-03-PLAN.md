---
phase: 06-cuad-classification
plan: 03
type: execute
wave: 3
depends_on: ["06-01", "06-02"]
files_modified:
  - inngest/functions/analyze-nda.ts
autonomous: true

must_haves:
  truths:
    - "Classification results persist to chunkClassifications table"
    - "Risk-scorer receives ClassifiedClause[] via compatibility shim"
    - "Batch classification uses separate Inngest steps with rate limiting"
    - "Both main pipeline and post-OCR pipeline use enhanced classifier"
  artifacts:
    - path: "inngest/functions/analyze-nda.ts"
      provides: "Pipeline with batch classification steps, persistence, and compat shim"
      contains: "chunkClassifications"
  key_links:
    - from: "inngest/functions/analyze-nda.ts"
      to: "db/schema/analyses.ts"
      via: "Insert into chunkClassifications table"
      pattern: "chunkClassifications"
    - from: "inngest/functions/analyze-nda.ts"
      to: "agents/classifier.ts"
      via: "runClassifierAgent with rawClassifications output"
      pattern: "rawClassifications"
---

<objective>
Wire the enhanced batch classifier into the Inngest pipeline and persist multi-label classifications to the new `chunkClassifications` table.

Purpose: The pipeline currently runs the classifier as a single `step.run('classifier-agent')`. This plan replaces that with a multi-step flow: batch classification with rate limiting between batches, persistence to the new `chunkClassifications` table, and a compatibility shim that produces `ClassifiedClause[]` for the unchanged risk-scorer and gap-analyst agents.

Output: Updated `analyze-nda.ts` with enhanced classifier integration in both the main pipeline and post-OCR pipeline
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/06-cuad-classification/06-CONTEXT.md
@.planning/phases/06-cuad-classification/06-RESEARCH.md
@.planning/phases/06-cuad-classification/06-01-SUMMARY.md
@.planning/phases/06-cuad-classification/06-02-SUMMARY.md
@inngest/functions/analyze-nda.ts
@agents/classifier.ts
@db/schema/analyses.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace single classifier step with batch classification pipeline</name>
  <files>inngest/functions/analyze-nda.ts</files>
  <action>
Modify the classifier integration in `analyze-nda.ts` for BOTH the main pipeline (`analyzeNda`) and the post-OCR pipeline (`analyzeNdaAfterOcr`).

**Import the new table:**
```typescript
import { chunkClassifications } from '@/db/schema/analyses'
```

**Replace the single `classifier-agent` step** (currently `step.run('classifier-agent', () => runClassifierAgent({...}))`) with the following multi-step flow:

```
1. classifier-agent (batch classification via enhanced runClassifierAgent)
2. persist-classifications (save rawClassifications to chunkClassifications table)
3. build-classifier-compat (not needed - classifier already returns clauses)
```

Actually, the enhanced `runClassifierAgent` from Plan 02 already handles batching internally (it loops over chunks in batches and calls the LLM multiple times). But this is problematic because `step.run()` makes the entire classifier agent a single step - if it fails partway through batch processing, ALL batches retry.

**Better approach: Keep classifier as single step, add persistence step after.**

The enhanced `runClassifierAgent` handles its own batching internally (multiple LLM calls within one invocation). This is acceptable because:
- The BudgetTracker tracks usage across all calls
- findSimilarClauses results are cached (LRU, 5-min TTL)
- If the step fails, retrying re-classifies everything (idempotent via ON CONFLICT)
- Splitting each batch into separate Inngest steps would require serializing chunk data between steps

**Changes to make:**

1. **Keep the classifier call** but use the enhanced return value:
```typescript
// Step: Classifier Agent (enhanced with batch classification)
const classifierResult = await step.run('classifier-agent', () =>
  runClassifierAgent({
    parsedDocument: classifierDocument,
    budgetTracker,
  })
)
```

2. **Add a new step immediately after** to persist classifications to the new table:
```typescript
// Step: Persist multi-label classifications to chunkClassifications table
await step.run('persist-classifications', async () => {
  // Build insert rows from rawClassifications
  const values: Array<{
    tenantId: string
    analysisId: string
    chunkId: string
    documentId: string
    category: string
    confidence: number
    isPrimary: boolean
    rationale: string | null
    chunkIndex: number
    startPosition: number | undefined
    endPosition: number | undefined
  }> = []

  // Map rawClassifications back to chunk data
  // rawClassifications has chunkIndex relative to the classifier input chunks
  const chunks = classifierDocument.chunks

  for (const result of classifierResult.rawClassifications) {
    const chunk = chunks[result.chunkIndex]
    if (!chunk) continue

    // Primary classification (always included)
    values.push({
      tenantId,
      analysisId,
      chunkId: chunk.id,
      documentId,
      category: result.primary.category,
      confidence: result.primary.confidence,
      isPrimary: true,
      rationale: result.primary.rationale,
      chunkIndex: chunk.index,
      startPosition: chunk.startPosition,
      endPosition: chunk.endPosition,
    })

    // Secondary classifications (only if confidence >= 0.3)
    for (const sec of result.secondary) {
      if (sec.confidence >= 0.3) {
        values.push({
          tenantId,
          analysisId,
          chunkId: chunk.id,
          documentId,
          category: sec.category,
          confidence: sec.confidence,
          isPrimary: false,
          rationale: null,
          chunkIndex: chunk.index,
          startPosition: chunk.startPosition,
          endPosition: chunk.endPosition,
        })
      }
    }
  }

  // Batch insert with conflict handling (idempotent)
  const DB_BATCH = 100
  for (let i = 0; i < values.length; i += DB_BATCH) {
    const batch = values.slice(i, i + DB_BATCH)
    await ctx.db.insert(chunkClassifications)
      .values(batch)
      .onConflictDoNothing()
  }
})
```

3. **The existing `classifierResult.clauses` continues to the validation gate and risk-scorer unchanged.** No compatibility shim needed because Plan 02 already builds the filtered `clauses` array.

4. **Update the classifier validation gate** to use `classifierResult.clauses` (unchanged - it already does this).

5. **Update progress message** after classification to include classification counts:
```typescript
await emitProgress(
  'classifying',
  50,
  `Classified ${classifierResult.clauses.length} clauses (${classifierResult.rawClassifications.length} total classifications)`
)
```

**Apply the same changes to `analyzeNdaAfterOcr`** (the post-OCR pipeline). The classifier and persistence steps should be identical.

**IMPORTANT:** The `rawClassifications` array from `classifierResult` uses `chunkIndex` as the index within the batch, NOT the global chunk index. In the classifier agent (Plan 02), each batch of chunks has indices 0, 1, 2, 3 within the batch. When persisting, you need to map these back to the original chunks. The Plan 02 classifier returns `rawClassifications` where each item's `chunkIndex` corresponds to the position in the original `parsedDocument.chunks` array (the classifier maps batch-local indices back to global indices before returning). Verify this is the case when integrating.

**Rate limiting:** The existing `step.sleep('rate-limit-classifier', ...)` AFTER the classifier step is still appropriate. The classifier internally handles rate limiting between its batch LLM calls if needed, but Inngest-level rate limiting between steps protects the overall pipeline.
  </action>
  <verify>Run `pnpm build` and verify compilation. Check that both `analyzeNda` and `analyzeNdaAfterOcr` have the `persist-classifications` step. Verify the import of `chunkClassifications` from `@/db/schema/analyses`.</verify>
  <done>Both pipelines persist multi-label classifications to `chunkClassifications` table after classifier runs. Risk-scorer and gap-analyst continue to receive `ClassifiedClause[]` unchanged. Insert uses `onConflictDoNothing` for idempotency.</done>
</task>

</tasks>

<verification>
1. `pnpm build` passes without errors
2. `chunkClassifications` import present in `analyze-nda.ts`
3. `persist-classifications` step exists in both `analyzeNda` and `analyzeNdaAfterOcr`
4. Classifier validation gate still uses `classifierResult.clauses` (unchanged)
5. Risk-scorer step still receives `classifierResult.clauses` (unchanged)
6. `onConflictDoNothing` used for idempotent classification inserts
7. Progress message updated to show total classification count
</verification>

<success_criteria>
- Multi-label classifications persist to `chunkClassifications` table
- Pipeline continues to work end-to-end (parser -> chunker -> classifier -> risk-scorer -> gap-analyst)
- Risk-scorer receives same `ClassifiedClause[]` interface (no breaking changes)
- Classification inserts are idempotent (safe for Inngest retries)
- Both main and post-OCR pipelines updated consistently
</success_criteria>

<output>
After completion, create `.planning/phases/06-cuad-classification/06-03-SUMMARY.md`
</output>
